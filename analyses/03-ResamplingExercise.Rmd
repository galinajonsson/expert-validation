---
title: "02-ResamplingExercise"
author: "Galina M. Jönsson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
<br />

## Introduction

<br />
Effort was made to ensure that the experts represented different professional backgrounds (research, conservation, curation, etc.) to limit systematic bias by averaging out individual subjectivity (Morgan, 2014). We approached the experts via email and invited them to participate in this review. Ten experts agreed to sit on the evaluation panel.    

Given the need to recruit multiple experts representing different individual and professional backgrounds, and that the total pool of candidate experts was felt to be small, ensuring high expert recruitment rates was a priority to limit systematic bias by averaging out individual subjectivity. We approached the experts via email and invited them to participate in this review. Ten experts agreed to sit on the evaluation panel.    

There was much variation between individual experts’ mean scores and variances **(Figure 4.A)**. Humans are not perfect statistical processors, and when facing uncertainty tend unconsciously to use cognitive heuristics. As a result, opinions are often biased by, for example, attaching undue weight to the evidence most recently encountered . An in-depth review of all potential sources of bias was not within the scope of our study. 


**Aims of exercise**
We did, however, 



*I think you can draw a strong conclusion from this. A conclusion that would give the paper something more concrete in terms of a methodological advance.

It is this: If there had been fewer than 10 experts, the scores would have been different. So 10 is good rule of thumb for this kind of exercise.

You could back this up with a resampling exercise. For n %in% 2:10 select n experts with replacement and recalculate the scores. Calculate one metric about the inter-expert variation and see how this settles down as n increases. Do this 100 times.*



<br/>





The experts scored both questions on five-point Likert scales (ranging from “strongly agree” to “strongly disagree”) to capture information on experts’ degree of confidence. Below, I re-format the scores numerically from -2 (Strongly disagree) to +2 (Strongly agree). 


```{r scale-raw-data, eval=TRUE, cache=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
### Load required package(s)
require(tidyr); require(dplyr)
### Read collated raw expert scores
df <- read.csv("../data/ExperstScores_Raw.csv")
### re-code responses from -2 (strongly disagree) to +2 (strongly agree)
df$score <- df$score-3
# specify columns to include (i.e., exclude com_name)
df <- df[,c("spp_name", "ID", "question", "score")]
### specify variable "ID" as discrete factor and order level 
#df$ID <- as.factor(as.character(df$ID))
#df$ID <- ordered(df$ID, levels = c("1", "2", "3", "4", "5", "6", "7",
#                                     "8", "9", "10"))
```
<br/>
<br/>


For n %in% 2:10 select n experts with replacement and recalculate the scores. Calculate one metric about the inter-expert variation and see how this settles down as n increases. Do this 100 times.*

```{r resample-scores, eval=TRUE, cache=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
### Create empty df to fill with two added columns for number of experts re-sampled and sample number
df.resamp <- data.frame(spp_name = as.character(),
                        ID = as.integer(),
                        question  = as.character(),
                        score = as.numeric(),
                        ID_sample = as.numeric(),
                        sample = as.numeric(),
                        stringsAsFactors=FALSE) 

### Add two empty columns for number of experts re-sampled and sample number to raw scores
df$ID_sample <- as.numeric(NA)
df$sample <- as.numeric(NA)


### Loop through for each of 2 to 10 experts
for(i in 2:10) {
  ### And for each, do the below 100 times
  for(j in 1:100){
    n <- sample(1:10, i, replace=T) # select i experts with replacement
    # Create a temporary df.resamp to populate
    df.resamp_temp <-  data.frame(spp_name = as.character(),
                        ID = as.integer(),
                        question  = as.character(),
                        score = as.numeric(),
                        ID_sample = as.numeric(),
                        sample = as.numeric(),
                        stringsAsFactors=FALSE) 
    ### Subset each of the i experts sampled above
    for(k in 1:length(n)){
      df.resamp_temp <- rbind(df.resamp_temp, subset(df, ID == n[k]))
      }
    df.resamp_temp$ID_sample <- i # Specify the number of experts sampled (2-10)
    df.resamp_temp$sample <- j # Specify the sampling number (1-100)
    df.resamp <- rbind(df.resamp, df.resamp_temp) # Add to final dataset
  }
}

### Save resampled expert scores as csv.file
#write.csv(df.resamp, file="../outputs/df_resamp.csv", row.names=F)
```




*For n %in% 2:10 select n experts with replacement and recalculate the scores. Calculate one metric about the inter-expert variation and see how this settles down as n increases. Do this 100 times.*




