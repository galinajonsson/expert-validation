---
title: "02-ResamplingExercise"
author: "Galina M. Jönsson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
<br />

## Introduction

<br />
Effort was made to ensure that the experts represented different professional backgrounds (research, conservation, curation, etc.) to limit systematic bias by averaging out individual subjectivity (Morgan, 2014). We approached the experts via email and invited them to participate in this review. Ten experts agreed to sit on the evaluation panel.    

Given the need to recruit multiple experts representing different individual and professional backgrounds, and that the total pool of candidate experts was felt to be small, ensuring high expert recruitment rates was a priority to limit systematic bias by averaging out individual subjectivity. We approached the experts via email and invited them to participate in this review. Ten experts agreed to sit on the evaluation panel.    

There was much variation between individual experts’ mean scores and variances **(Figure 4.A)**. Humans are not perfect statistical processors, and when facing uncertainty tend unconsciously to use cognitive heuristics. As a result, opinions are often biased by, for example, attaching undue weight to the evidence most recently encountered . An in-depth review of all potential sources of bias was not within the scope of our study. 


**Aims of exercise**
We did, however, 



*I think you can draw a strong conclusion from this. A conclusion that would give the paper something more concrete in terms of a methodological advance.

It is this: If there had been fewer than 10 experts, the scores would have been different. So 10 is good rule of thumb for this kind of exercise.

You could back this up with a resampling exercise. For n %in% 2:10 select n experts with replacement and recalculate the scores. Calculate one metric about the inter-expert variation and see how this settles down as n increases. Do this 100 times.*



<br/>
