---
title: "02-DataAnalysis"
author: "Galina M. JÃ¶nsson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
<br />

# Introduction

The purpose of the expert review process is to determine whether the long-term resident butterfly trends produced by our occupancy model formulation are a plausible description of how species changed between 1900-1976, and reflects existing knowledge for species post-1976.
<br />
The current script contains all code and results for the statistical analyses of the experts' scores. Namely, we fit a linear mixed-effects model to seek more definitive answers to the questions that emerged whilst exploring the results (see "01-ExploreResults.Rmd" for details).
<br />
We use a mixed effects model to test whether time period (i.e., 1900-1976 vs. 1976-2016) effects expert scores (Agresti, 2002; Gibson et al., 2011; Norman, 2010); in other words, whether the experts have scored the two periods differently. We therefore fit the model with time period as a fixed effect, and both taxon and respondent ID as random effects. 
<br />
<br />
<br />

# Model Fitting & Selection

First, we determine which distribution family best explains the distribution of our raw data (i.e., the expert scores). To do so, we fit two linear mixed-effects model with identical fixed and random effect structures, but different distributions: Normal (Gaussian) and Poisson. The models are fitted with time period as a fixed effect, and both taxon and respondent ID as random effects. We use analysis-of-variance (anova) to test the models against one another. Note that we do not re-code the expert response scores from the default scale (1-5) to ranging from -2 (strongly disagree) to +2 (strongly agree) because Poisson cannot handle negative values.    

<br />
```{r read-data-generate-models, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE, cache=FALSE}
### Load required package(s)
require(lme4)

####################### Read and tidy raw collated data
### Read collated raw expert scores
df <- read.csv("../data/ExperstScores_Raw.csv")
# Subset relevant data columns and give "question" more informative names
df <- na.omit(df[, c("spp_name", "ID", "question", "score")])
# Give "question" more informative names
names(df)[names(df) == "question"] <- "Time_Period"
df[df$'Time_Period'=="q1", "Time_Period"] <- "1900-1976"
df[df$'Time_Period'=="q2", "Time_Period"] <- "1976-2015"


####################### Fit competing linear mixed-effects models with different distributions
# Normal (Gaussian) 
lmerNormal <- lmer(score ~ Time_Period + (1|spp_name) + (1|ID), data = df)
# Poisson
glmerPoisson <- glmer(score ~ Time_Period + (1|spp_name) + (1|ID), 
                      data = df, family = poisson)

# compare models
anova(glmerPoisson, lmerNormal)
```
<br/>
The model comparison shows that the normal (Gaussian) model has a significantly better fit to the data compared with the Poisson model. The analysis of variance table for the two fitted model objects shows their AIC, BIC, and LogLik scores, showing:

* The the normal (Gaussian) model shows lower values for AIC and BIC, which indicate better model fit. For AIC and BIC, a general rule of thumb is that more than 2 unit differences between models are considered different.    
* Similarly, the LogLik score for the normal (Gaussian) model has a less negative value, indicating better fit.    
* Although the $x^{2}$ test isn't fully appropriate, nor reliable (more details below), for mixed effects models, the very small p-value (<< 0.05) is indicative of a real difference between the two models.   

We will therefore continue with the normal (Gaussian) model. 

<br />
<br />
<br />

# Testing mixed models parameters

*Fixed effects represent an effect that if we draw many samples, the effect would be consistent across samples (Winter 2019) while random effects should vary for each new sample that is drawn. Random effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical. Random effects represent a sample of an infinite number of possible levels, whilst fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels.*

*Mixed model parameters do not have nice asymptotic distributions to test against. (i.e., the parameters do not asymptotically converge to known distributions), complicating the inferences which can be made from such models. One source of the complexity is a penalty factor (shrinkage) which is applied to the random effects in the calculation of the likelihood (or restricted likelihood) function the model is optimized to. This results in distributions which are no longer chi squared or F. This penalty factor also complicates determining the degrees of freedom to associate with the estimate of a random effect.* Therefore, we use both visual and quantitative testes for the fixed effect, but only vial diagnostics for the random effects.
<br/>
<br/>
<br/>

## Time period (fixed) effect

First, we re-code response scores back to the -2 to 2 scale because normal (Gaussian) models can handle negative values. We then inspect the model output. See below for testing whether the fixed effect is significant (i.e., whether scores for the two time periods differ) and plot the results. 
<br/>
```{r fixed-effect-plot, message=FALSE, eval=TRUE, warning=FALSE, echo=TRUE, cache=FALSE}
### Load required package(s)
require(lme4); require(car); require(effects); require(glmmTMB)

### re-code responses from -2 (strongly disagree) to +2 (strongly agree)
df$score <- df$score-3

# Re-generate the Normal (Gaussian) models with re-coded scores
lmerNormal <- lmer(score ~ Time_Period + (1|spp_name) + (1|ID), data = df)

# inspect results
summary(lmerNormal)

# Plot results
effects_ok <- (requireNamespace("effects") && getRversion() >= "3.6.0")
if (effects_ok) {
plot(allEffects(lmerNormal), main= "Figure A: Time period effect plot" )
}
```
<br/>
The model summary output indicates that scores differ between the two time periods such that pre-1976 trends are lower than post-1976 scores. Similarly, the effect plots, including their non-overlapping error bars, agree.
<br/>
<br/>


### Hypothesis testing: Time period effect

We explicitly test whether scores differ between the two time periods using a type II Wald $x^{2}$ test.
<br/>
```{r test-fixed-effect, message=FALSE, eval=TRUE, warning=FALSE, echo=TRUE, cache=FALSE}
car::Anova(lmerNormal)
```
<br/>
The type II Wald $x^{2}$ square test confirms that scores for Pre-1976 trends are significantly lower than post-1976 scores: on average, pre-1976 trend scores are 0.70428 lower than post-1976 scores.    

We now look into a few different model diagnostics to decipher what's up with our random effect. 
<br/>
<br/>
<br/>

## Species ID (random) effect

**Useful resources:**
* https://slcladal.github.io/regression.html#2_Mixed-Effects_Regression  
* Book: J. C. Pinheiro & D. M. Bates. 2000. *Mixed-Effects Models in S and S-PLUS*, **Chapter 1.1**. Springer.

<br/>
Random Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope, mixed-effects models can therefore have various random intercepts or various random slopes, or both. Here, we will only focus on the random intercepts because this is the more common method.
<br/>

First, we extract the conditional modes of the random effects from the fitted model object. As our fitted model is a linear mixed model, the conditional modes are also the conditional means.<br/>
Second, we use the extracted modes to generate diagnostic plots focusing on the random effect structure of species ID here (see below for participant ID): <br/>

* Cleveland dot plot, showing the estimated random effects (coefficients) grouped according to all combinations of the levels of the factors (here, species ID).    
* Quantile-quantile plots of a sample against a theoretical distribution.   
<br/>
```{r diagnostics1-spp, message=FALSE, eval=TRUE, echo=TRUE, cache=FALSE, fig.show="hold"}
# Load required package(s)
require(ggplot2); require(lattice)

# Extract the modes of the random effect spp_name
rr1 <- ranef(lmerNormal)

##################### Cleveland dot plot
## Collate data
dd <- as.data.frame(rr1)
dd <- dd[dd$'grpvar'=="spp_name",]
# Plot
ggplot(dd, aes(y=grp,x=condval)) +
  geom_point() + facet_wrap(~term,scales="free_x") +
  geom_errorbarh(aes(xmin=condval -2*condsd,
                     xmax=condval +2*condsd), height=0) +
  labs(x = "Value of Conditional Mean", y = "Species",
       title = "Figure B: Species ID cleveland dot plot",
       subtitle = "Random effect of the intercepts") 


##################### QQ plot
## Collate data
qq <- as.data.frame(qqmath(rr1)$spp_name$panel.args[[1]])
qq <- unique(merge(qq, dd[,c(2,4,5)], by.x = "x", by.y = "condval", all.x = TRUE, all.y = FALSE))
# Plot
ggplot(qq, aes(y=y,x=x)) +
  geom_point() + geom_errorbarh(aes(xmin=x -2*condsd,
                     xmax=x +2*condsd)) +
  facet_wrap(~term,scales="free_x") +
  labs(x = "Value of Conditional Mean", 
	     y = "Standard normal qunatiles",
       title = "Figure C: Species ID QQ plot",
       subtitle = "Random effect of the intercepts") 
```
<br />
**General trends:** There appears to be differences among species. Some species look as if some are generally 'easier' whilst others are 'harder' to model for both time periods as evident from some scoring high and other low across periods and participants. These are admittedly hard to see on these plots so let's move on. 

<br/>
```{r diagnostics2-spp, message=FALSE, eval=TRUE, echo=TRUE, cache=FALSE}
plot(lmerNormal, spp_name ~ resid(.), abline = 0, 
     ylab ="Species", xlab = "Residuals") # generate diagnostic plots
```
<br/>
The plot above shows that there are some outliers (points outside the boxes) and that the variability differs between species.

<br/>
<br/>
<br/>

## Participant ID (random) effect
<br/>
Here, we repeat the visual inspection/diagnostic plots produced for species ID (above) for the random effect participant ID. 
<br/>
```{r diagnostics1-id, message=FALSE, eval=TRUE, echo=TRUE, cache=FALSE, fig.show="hold"}
# Load required package(s)
require(ggplot2); require(lattice)

# Extract the modes of the random effect ID
rr1 <- ranef(lmerNormal)

##################### Cleveland dot plot
## Collate data
dd <- as.data.frame(rr1)
dd <- dd[dd$'grpvar'=="ID",]
# Plot
ggplot(dd, aes(y=grp,x=condval)) +
  geom_point() + facet_wrap(~term,scales="free_x") +
  geom_errorbarh(aes(xmin=condval -2*condsd,
                     xmax=condval +2*condsd), height=0) +
  labs(x = "Value of Conditional Mean", y = "Participant ID",
       title = "Figure D: Participant ID cleveland dot plot",
       subtitle = "Random effect of the intercepts") 


##################### QQ plot
## Collate data
qq <- as.data.frame(qqmath(rr1)$ID$panel.args[[1]])
qq <- unique(merge(qq, dd[,c(2,4,5)], by.x = "x", by.y = "condval", all.x = TRUE, all.y = FALSE))
# Plot
ggplot(qq, aes(y=y,x=x)) +
  geom_point() + geom_errorbarh(aes(xmin=x -2*condsd,
                     xmax=x +2*condsd)) +
  facet_wrap(~term,scales="free_x") +
  labs(x = "Value of Conditional Mean", 
	     y = "Standard normal qunatiles",
       title = "Figure E: Participant ID QQ plot",
       subtitle = "Random effect of the intercepts") 
```
<br />
**General trends:** There appears to be differences among participants. Two extremes are 09 and 06, which tend to score low and high, respectively, across species and time series. 

<br/>
```{r diagnostics2, message=FALSE, eval=TRUE, echo=TRUE, cache=FALSE}
plot(lmerNormal, ID ~ resid(.), abline = 0, 
     ylab ="Participant ID", xlab = "Residuals") # generate diagnostic plots
```
<br/>
<br/>
The plot above show that there are some outliers (points outside the boxes) and that the variability differs between participants.

* Note the -3 residual for participant 05 (suspected mistake; Figure 6). 
* Neither participant 04 nor 10 (also suspected mistakes; Figure 6) look like they have 'extreme' residuals. <br/>

We therefore examine both participants' and species' standardized residuals (or Pearsonâs residuals) versus fitted values in isolation.
<br/>
```{r diagnostics3-id, message=FALSE, eval=TRUE, echo=TRUE, cache=FALSE}
plot(lmerNormal, resid(., type = "pearson") ~ fitted(.) | ID, id = 0.05, 
     adj = -0.3, pch = 20, col = "gray40", 
     ylab ="Standardized (Pearsonâs) Residuals", xlab = "Fitted Values")
```
<br/>

The plots show the standardized residuals (or Pearsonâs residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots); admittedly, this is rather hard to see.


***